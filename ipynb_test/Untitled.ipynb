{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc0f48f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccac0f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "def load_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    # return jsonl data\n",
    "    return data\n",
    "\n",
    "def _get_discussion(soup):\n",
    "    discussions = soup.find_all('div',class_='relative h-16 w-full overflow-hidden')\n",
    "    if discussions is None:\n",
    "        return []\n",
    "    return discussions\n",
    "\n",
    "def _get_discussion_href(discussions_list, model_id):\n",
    "    href_list = []\n",
    "    for discussion in discussions:\n",
    "\n",
    "        svg = discussion.find('svg')\n",
    "        svg_classes = svg['class']\n",
    "        if 'viewbox' in svg.attrs:\n",
    "            view_box = svg['viewbox']\n",
    "        else:\n",
    "            view_box= None\n",
    "        # class\n",
    "        # text-green-600 = commit\n",
    "        # text-blue-600 = issue data\n",
    "        # text-purple-600 = closed issue data\n",
    "        # view box attibute 0 0 32 32\n",
    "        if ('text-blue-600' in svg_classes or 'text-purple-600' in svg_classes) and view_box=='0 0 32 32' :\n",
    "\n",
    "            title = discussion.find('h4').text.strip()\n",
    "            href = discussion.find('a')['href']\n",
    "            discussion_id = href.split('/')[-1]\n",
    "            author = discussion.find('span',class_='contents').find('a')['href'][1:]\n",
    "            href_list.append(\n",
    "                {\"ID\":f'{model_id}-{discussion_id}',\n",
    "                '主題':title,\n",
    "                '来源':author,\n",
    "                '回复':[],\n",
    "                '元数据':{'href':href}\n",
    "                }\n",
    "            )\n",
    "    print('href_list',href_list)\n",
    "    return href_list\n",
    "\n",
    "def get_comment(discussion_obj_list):\n",
    "    for discussion_obj in discussion_obj_list:\n",
    "        href = discussion_obj['元数据']['href']\n",
    "        res = requests.get(f\"https://huggingface.co{href}\")\n",
    "        soup = BeautifulSoup(res.text,'html.parser')\n",
    "        conversations = soup.find_all('div',class_=re.compile('scroll-mt-4'))\n",
    "        \n",
    "        for i,conversation in enumerate(conversations):\n",
    "            blockquote = ('\\n').join([convs.text for convs in conversation.select('blockquote p')])\n",
    "            try:\n",
    "                comment = ('\\n').join([convs.text for convs in conversation.select('div > p')])\n",
    "            except Exception as e:\n",
    "                comment = conversation.find('span',class_='peer italic text-gray-400').text\n",
    "            reply_time = conversation.find('time')['datetime']\n",
    "            try:\n",
    "                author = conversation.find('div',class_='mr-2 flex flex-shrink-0 items-center').find('a').text\n",
    "            except Exception as e:\n",
    "                author = conversation.find('div',class_='mr-2').text\n",
    "            discussion_obj['回复'].append(\n",
    "                {\n",
    "                    \"楼ID\":str(i),\n",
    "                    \"回复\":comment,\n",
    "                    \"扩展字段\":{\n",
    "                        \"回复人\":author,\n",
    "                        \"引用内容\":blockquote,\n",
    "                        \"回复时间\":reply_time\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "            \n",
    "    return discussion_obj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f46a6ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'temp/datasets_20241102.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m repo_list \u001b[38;5;241m=\u001b[39m \u001b[43mload_jsonl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtemp/datasets_20241102.jsonl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m repo \u001b[38;5;129;01min\u001b[39;00m repo_list:\n\u001b[1;32m      3\u001b[0m     repo_id \u001b[38;5;241m=\u001b[39m repo[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mload_jsonl\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_jsonl\u001b[39m(file_path):\n\u001b[1;32m      4\u001b[0m     data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[1;32m      7\u001b[0m             data\u001b[38;5;241m.\u001b[39mappend(json\u001b[38;5;241m.\u001b[39mloads(line))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'temp/datasets_20241102.jsonl'"
     ]
    }
   ],
   "source": [
    "repo_list = load_jsonl('temp/datasets_20241102.jsonl')\n",
    "for repo in repo_list:\n",
    "    repo_id = repo['id']\n",
    "    res = requests.get(f\"https://huggingface.co/datasets/{repo_id}/discussions?status=open\")\n",
    "    soup = BeautifulSoup(res.text,'html.parser')\n",
    "    discussions = _get_discussion(soup)\n",
    "    \n",
    "    res = requests.get(f\"https://huggingface.co/datasets/{repo_id}/discussions?status=close\")\n",
    "    soup = BeautifulSoup(res.text,'html.parser')\n",
    "    discussions += _get_discussion(soup)\n",
    "    discussion_obj_list = _get_discussion_href(discussions, repo_id)\n",
    "    conversations = get_comment(discussion_obj_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9706bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "href_list []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "repo_id='neuralwork/arxiver'\n",
    "res = requests.get(f\"https://huggingface.co/datasets/{repo_id}/discussions?status=open\")\n",
    "soup = BeautifulSoup(res.text,'html.parser')\n",
    "open_discussions = _get_discussion(soup)\n",
    "res = requests.get(f\"https://huggingface.co/datasets/{repo_id}/discussions?status=close\")\n",
    "soup = BeautifulSoup(res.text,'html.parser')\n",
    "close_discussions = _get_discussion(soup)\n",
    "print(open_discussions)\n",
    "print(close_discussions)\n",
    "discussion_obj_list = _get_discussion_href(discussions, repo_id)\n",
    "conversations = get_comment(discussion_obj_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44ea2899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7516efd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
