[{"ID": "PleIAs/common_corpus-6", "主題": "Collection tags (i.e. OpenGovernment)", "来源": "EvkoGS", "回复": [{"楼ID": "0", "回复": "Hi! Extremely impressive job, huge respect for doing it!I'd like to use OpenGovernment subset of this dataset, but I see many other collection names like USPTO, etc., can you please add more broad tags? Publishing description for collections would work as well! I would include manually everything I need.\nThanks!", "扩展字段": {"回复人": "EvkoGS", "引用内容": "", "回复时间": "2024-11-15T23:53:08"}}], "元数据": {"href": "/datasets/PleIAs/common_corpus/discussions/6"}}, {"ID": "PleIAs/common_corpus-5", "主題": "How many GB/TB?", "来源": "BBLL3456", "回复": [{"楼ID": "0", "回复": "Hi,\nMay i know the total size of this corpus?", "扩展字段": {"回复人": "BBLL3456", "引用内容": "", "回复时间": "2024-11-15T05:16:19"}}], "元数据": {"href": "/datasets/PleIAs/common_corpus/discussions/5"}}, {"ID": "PleIAs/common_corpus-4", "主題": "Is Common Corpus Pre-Shuffled?", "来源": "Avelina", "回复": [{"楼ID": "0", "回复": "Is Common Corpus pre-shuffled? And if so, is it inter-collection shuffled (i.e. each line is selected from a random collection), intra-collection shuffled (i.e. each collection itself is shuffled), or both?\nIt appears to be pre-shuffled, but clarification from the maintainers would be appreciated!", "扩展字段": {"回复人": "Avelina", "引用内容": "", "回复时间": "2024-11-14T18:22:39"}}], "元数据": {"href": "/datasets/PleIAs/common_corpus/discussions/4"}}, {"ID": "PleIAs/common_corpus-3", "主題": "[bot] Conversion to Parquet", "来源": "parquet-converter", "回复": [{"楼ID": "0", "回复": "The parquet-converter bot has created a version of this dataset in the Parquet format in the refs/convert/parquet branch.\nApache Parquet is a popular columnar storage format known for:\nThis is what powers the dataset viewer on each dataset page and every dataset on the Hub can be accessed with the same code (you can use HF Datasets, ClickHouse, DuckDB, Pandas or Polars, up to you).\nYou can learn more about the advantages associated with Parquet in the documentation.\nYou can access the Parquet version of the dataset by following this link: refs/convert/parquet\nWhen the dataset is already in Parquet format, the data are not converted and the files in refs/convert/parquet are links to the original files. This rule has an exception to ensure the dataset viewer API to stay fast: if the row group size of the original Parquet files is too big, new Parquet files are generated.\nYou don't need to do anything. The Parquet version of the dataset is available for you to use. Refer to the documentation for examples and code snippets on how to query the Parquet files with ClickHouse, DuckDB, Pandas or Polars.\nIf you have any questions or concerns, feel free to ask in the discussion below. You can also close the discussion if you don't have any questions.", "扩展字段": {"回复人": "parquet-converter", "引用内容": "", "回复时间": "2024-11-14T00:10:18"}}], "元数据": {"href": "/datasets/PleIAs/common_corpus/discussions/3"}}, {"ID": "PleIAs/common_corpus-2", "主題": "How to filter for language?", "来源": "Rijgersberg", "回复": [{"楼ID": "0", "回复": "Congratulations and big thanks on this monumental data release!\nThe model card mentions that the language of the text is included in the metadata. However, I am not able to find it. Can you point to exactly where the language metadata is stored?", "扩展字段": {"回复人": "Rijgersberg", "引用内容": "The dataset is multilingual. The language text is included in the metadata, so data can be filtered by language. Additionally, some of the text data are historical. The year each text is written is included in the metadata, therefore it is possible to construct a dataset with a custom date cutoff if desired.", "回复时间": "2024-11-13T21:02:44"}}, {"楼ID": "1", "回复": "Hi! Yes was planned for this release (along with additional metadata, including title/author for document where it is relevant). Will be added very soon.", "扩展字段": {"回复人": "Pclanglais", "引用内容": "", "回复时间": "2024-11-13T22:10:10"}}], "元数据": {"href": "/datasets/PleIAs/common_corpus/discussions/2"}}, {"ID": "PleIAs/common_corpus-1", "主題": "Clarification regarding Wiki content", "来源": "mjbommar", "回复": [{"楼ID": "0", "回复": "Would Wikimedia please be able to comment on their participation in this project?  Below is the full text of WMF Legal team's response to our request for permission to pretrain:\nFrom: Eloísa Granado egranado-ctr@wikimedia.orgSent: Fri, 22 Mar 2024 12:28:43 -0700 (PDT)Hi Michael, \nThanks for reaching out! Apologize for the delay. I was consulting with our legal department to give you the best answer I can. We’ve discussed this topic internally for the better part of the last year, so my apologies if this is a lengthy email.\nBefore talking about the legal aspects around compliance with the open licenses, here’s the simplest way to understand our perspective. New additions to Wikipedia rely on active volunteers, and these volunteers come from everyday readers who learn they add to Wikipedia because they follow attribution links back to our site.\nSo fundamentally, proper attribution from bulk reusers like yourselves ensures that Wikipedia can continue to exist long-term. This is especially important in the context of LLM training where there’s no direct throughline between the Wikipedia data you use and providing a link back to Wikipedia that the end user can see in the output.\nI’ll try to address your question below, but I may bring up more issues than solutions here.  This is not legal advice since compliance with the licenses is first and foremost a legal issue between you and your legal team. What I’ll be describing are some general principles as well as Wikimedia’s ideals when it comes to reuse.\n1. Providing a general notice to customers would not be an adequate solution to compliance. While a positive first step, supplying a notice document to “customers” would likely not fully meet the requirements for compliance. This is for a variety of reasons but chief among them: the notice would need to be made to everyone the content is shared with, not just customers. Moreover, the visibility of the notice would need to follow the Creative Commons criteria of being “reasonable to the medium.”\nAs an FYI, Wikipedia's licenses are primarily Creative Commons Attribution-ShareAlike (CC BY-SA) licenses. You can read the full text of the license here. \n2.Each piece of content in Wikimedia projects has its own specific license. A Wikipedia article is licensed by dozens of people but in theory, can be singularly attributed to Wikipedia as the Author under a CC-BY-SA 4.0 license (see above). Contrast that with Wikimedia Commons where every photo needs to be named with the individual Author and each photo’s individual license (anything from CC-BY 1.0 to CC-BY-SA 4.0) to be considered properly attributed. Wikinews uses a CC-BY-SA license but not 4.0. Wikimedia Commons is something we’re roadmapping at the moment to proactively address the larger, growing needs of the generative AI landscape around video/images/audio reuse. If you are interested, we can chat further about this.\nFinally, a small amount of content specifically on Wikipedia is published under fair use (not openly licensed). This is small but meaningful if the goal is to provide relatively unencumbered data. The list of nuances in our dataset is long but ultimately navigable if work is put in to do so.\n**I can continue, but I think this email is more useful for opening up questions than resolving them concretely. We are monitoring what many LLM companies do with Wikimedia data and generally to be upfront, many may not be compliant with the letter of the Creative Commons rules or the spirit of the licenses. **\nThat’s why we’re happy you’ve reached out to discuss this with us early on as you refine your business model. As part of the Wikimedia Enterprise team, we can help you explore opportunities where attribution gets simplified for reuse, including in an LLM context. \nOur legal department did express some practical concerns when I pointed them to this Wired Magazine article. It’s definitely forward-thinking that you’re attempting to create a fully public-domain database, but they described challenges in characterizing any training set that contains Wikimedia content as “public domain,” since there are many considerations for downstream licensees.\nFor example, downstream licensees would need to continue to adhere to the “attribution,” “share-alike,” and other elements of the license. To the extent you intend to include Wikimedia data in your set (or already have), our legal team suggested it would be very important to be in contact with them. Typically, my team can facilitate conversations like that since open licensing support is the kind of thing we assist Enterprise customers with. \nWikimedia Enterprise is a team established to help large commercial reusers ingest and make the best use of Wikimedia data. Our customers benefit from a worldwide, royalty-free license to use the Wikimedia Enterprise APIs to access Wikipedia project data. Under our contract, content provided through the API is still subject to the applicable Free Culture Licenses and Open Source Licenses but companies training LLMs who are existing customers have found our support useful since complexity is high, but ultimately not-a-barrier.\nI’m sharing our deck and roadmap with you for you to learn more about our offerings. Collaborative discussions like this are crucial for shaping an ecosystem that respects open knowledge while still fostering innovation. If you’d like to discuss this further, please let us know. We are happy to continue the conversation and explore potential solutions or partnerships that align with our values while enabling your important work. \nCheers,Eloisa GranadoWikimedia Enterprise", "扩展字段": {"回复人": "mjbommar", "引用内容": "", "回复时间": "2024-11-13T18:40:15"}}], "元数据": {"href": "/datasets/PleIAs/common_corpus/discussions/1"}}]
[{"ID": "microsoft/orca-agentinstruct-1M-v1-2", "主題": "[bot] Conversion to Parquet", "来源": "parquet-converter", "回复": [{"楼ID": "0", "回复": "The parquet-converter bot has created a version of this dataset in the Parquet format in the refs/convert/parquet branch.\nApache Parquet is a popular columnar storage format known for:\nThis is what powers the dataset viewer on each dataset page and every dataset on the Hub can be accessed with the same code (you can use HF Datasets, ClickHouse, DuckDB, Pandas or Polars, up to you).\nYou can learn more about the advantages associated with Parquet in the documentation.\nYou can access the Parquet version of the dataset by following this link: refs/convert/parquet\nWhen the dataset is already in Parquet format, the data are not converted and the files in refs/convert/parquet are links to the original files. This rule has an exception to ensure the dataset viewer API to stay fast: if the row group size of the original Parquet files is too big, new Parquet files are generated.\nYou don't need to do anything. The Parquet version of the dataset is available for you to use. Refer to the documentation for examples and code snippets on how to query the Parquet files with ClickHouse, DuckDB, Pandas or Polars.\nIf you have any questions or concerns, feel free to ask in the discussion below. You can also close the discussion if you don't have any questions.", "扩展字段": {"回复人": "parquet-converter", "引用内容": "", "回复时间": "2024-11-05T00:10:42"}}], "元数据": {"href": "/datasets/microsoft/orca-agentinstruct-1M-v1/discussions/2"}}]
