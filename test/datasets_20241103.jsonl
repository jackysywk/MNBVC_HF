[{"ID": "fka/awesome-chatgpt-prompts-51", "主題": "AI MTG League Manager", "来源": "adamf9898", "回复": [{"楼ID": "0", "回复": "\"AI MTG League Manager\", \"I want you to act like a professional game and web app developer. You will be a Magic: the Gathering League Manager and Instructor. You provide me with HTML, CSS, and Javascript based on my input. Do Not write explanations. You will reply with wise advice and meaningful guidance. Additionally, suggest practical methods for putting this advice into action with step-by-step instructions as lessons, quizes, simulations, exams, experiments, and presentations for each response. My first input is \"\"Generate a Scryfall combination of input searches for a Magic: The Gathering theme booster pack that is based on this description: ${queryInput.value}. output the results as 20 random selections as number of results and a card list of names only that is in a text area and as card normal images between headers with card name and footers with oracle text. Create a console log to log and display every action.  Create a dropdown menu in the upper left with options, settings, clear log, updates, links to sections, table of contents, index, MTG card search, MTG deck analysis and statistics, MTG lessons, MTG quizzes, MTG simulations, MTG formats, MTG forums, comments, blog posts, debug options, find problems, fix problems with best solutions, add more that you would agree with.\"\"\"", "扩展字段": {"回复人": "adamf9898", "引用内容": "", "回复时间": "2024-09-19T16:14:58"}}], "元数据": {"href": "/datasets/fka/awesome-chatgpt-prompts/discussions/51"}}, {"ID": "fka/awesome-chatgpt-prompts-49", "主題": "Request: DOI", "来源": "Amyww", "回复": [{"楼ID": "0", "回复": "？", "扩展字段": {"回复人": "Amyww", "引用内容": "", "回复时间": "2024-09-13T03:40:06"}}, {"楼ID": "1", "回复": "Words. Use them?", "扩展字段": {"回复人": "chokosims", "引用内容": "", "回复时间": "2024-09-19T19:57:37"}}], "元数据": {"href": "/datasets/fka/awesome-chatgpt-prompts/discussions/49"}}, {"ID": "fka/awesome-chatgpt-prompts-45", "主題": "awesome-huggingchat-prompts or awesome-llama-prompts", "来源": "clem", "回复": [{"楼ID": "0", "回复": "Thanks for this \n@fka\n! As more people are using open chat models, it would be awesome to have such ressource for them!", "扩展字段": {"回复人": "clem", "引用内容": "", "回复时间": "2024-09-03T13:27:25"}}, {"楼ID": "1", "回复": "\n@clem\n \n@fka\n We have the ShareLM plugin, but it requires some user setup https://sharelm.github.io/", "扩展字段": {"回复人": "burtenshaw", "引用内容": "Thanks for this \n@fka\n! As more people are using open chat models, it would be awesome to have such ressource for them!", "回复时间": "2024-09-03T13:34:03"}}, {"楼ID": "2", "回复": "\n@clem\n actually this dataset can be renamed into \"awesome-chat-prompts\" or smth more generic one since there are many chat models around and these prompts can be used with all of them, wdyt?\np.s. thanks for the sponsorship on gh!", "扩展字段": {"回复人": "fka", "引用内容": "", "回复时间": "2024-09-03T13:43:27"}}, {"楼ID": "3", "回复": "Not entirely sure but I suspect that part of your success comes from the SEO associated with the name \"awesome-chatgpt-prompts\" (reaching people who search specifically for this) so not sure I would change the name of the dataset versus creating new specific ones for other models. But take my feedback with bucket of salts because I don't have a lot of context.", "扩展字段": {"回复人": "clem", "引用内容": "", "回复时间": "2024-09-03T15:34:56"}}], "元数据": {"href": "/datasets/fka/awesome-chatgpt-prompts/discussions/45"}}, {"ID": "fka/awesome-chatgpt-prompts-32", "主題": "[bot] Conversion to Parquet", "来源": "parquet-converter", "回复": [{"楼ID": "0", "回复": "The parquet-converter bot has created a version of this dataset in the Parquet format in the refs/convert/parquet branch.\nApache Parquet is a popular columnar storage format known for:\nThis is what powers the Dataset Viewer on each dataset page and every dataset on the Hub can be accessed with the same code (you can use HF Datasets, ClickHouse, DuckDB, Pandas or Polars, up to you).\nYou can learn more about the advantages associated with Parquet in the documentation.\nYou can access the Parquet version of the dataset by following this link: refs/convert/parquet\nWhen the dataset is already in Parquet format, the data are not converted and the files in refs/convert/parquet are links to the original files. This rule has an exception to ensure the Datasets Server API to stay fast: if the row group size of the original Parquet files is too big, new Parquet files are generated.\nYou don't need to do anything. The Parquet version of the dataset is available for you to use. Refer to the documentation for examples and code snippets on how to query the Parquet files with ClickHouse, DuckDB, Pandas or Polars.\nIf you have any questions or concerns, feel free to ask in the discussion below. You can also close the discussion if you don't have any questions.", "扩展字段": {"回复人": "parquet-converter", "引用内容": "", "回复时间": "2024-03-15T00:10:49"}}], "元数据": {"href": "/datasets/fka/awesome-chatgpt-prompts/discussions/32"}}, {"ID": "fka/awesome-chatgpt-prompts-20", "主題": "JSONDecodeError: Expecting value: line 1 column 1 (char 0)", "来源": "texasdave2", "回复": [{"楼ID": "0", "回复": "Getting this error when running on prem.  I don't get it with other downloaded datasets, just this one...  \nany ideas?\nthanks!! ", "扩展字段": {"回复人": "texasdave2", "引用内容": "", "回复时间": "2023-10-18T00:25:14"}}], "元数据": {"href": "/datasets/fka/awesome-chatgpt-prompts/discussions/20"}}, {"ID": "fka/awesome-chatgpt-prompts-19", "主題": "Which 'text' column?", "来源": "LordNikon", "回复": [{"楼ID": "0", "回复": "Hi I'm just wondering which text column to train on. I'm using autotrain within a space I've created. Should I use 'prompt'?", "扩展字段": {"回复人": "LordNikon", "引用内容": "", "回复时间": "2023-09-29T17:06:36"}}], "元数据": {"href": "/datasets/fka/awesome-chatgpt-prompts/discussions/19"}}, {"ID": "fka/awesome-chatgpt-prompts-18", "主題": "test", "来源": "Namitoo", "回复": [{"楼ID": "0", "回复": "建议", "扩展字段": {"回复人": "Namitoo", "引用内容": "", "回复时间": "2023-09-13T06:53:16"}}, {"楼ID": "1", "回复": "س", "扩展字段": {"回复人": "saeedD7", "引用内容": "", "回复时间": "2024-01-15T11:31:59"}}], "元数据": {"href": "/datasets/fka/awesome-chatgpt-prompts/discussions/18"}}, {"ID": "fka/awesome-chatgpt-prompts-15", "主題": "Clustering awesome chatgpt prompts", "来源": "dfurman", "回复": [{"楼ID": "0", "回复": "Hi HF community,\nSharing some clustering experiments (https://github.com/daniel-furman/awesome-chatgpt-prompts-clustering/tree/main) I performed on this dataset. Seems to be split into 5 core themes with roughly equal proportions across the dataset. \nHappy data-ing!", "扩展字段": {"回复人": "dfurman", "引用内容": "", "回复时间": "2023-09-03T16:09:36"}}, {"楼ID": "1", "回复": "", "扩展字段": {"回复人": "deleted", "引用内容": "", "回复时间": "2023-09-06T07:44:42"}}], "元数据": {"href": "/datasets/fka/awesome-chatgpt-prompts/discussions/15"}}, {"ID": "fka/awesome-chatgpt-prompts-12", "主題": "Gaslighter doesn't work anymore :(", "来源": "bennyb0y", "回复": [{"楼ID": "0", "回复": "Sad", "扩展字段": {"回复人": "bennyb0y", "引用内容": "", "回复时间": "2023-07-21T18:08:52"}}], "元数据": {"href": "/datasets/fka/awesome-chatgpt-prompts/discussions/12"}}, {"ID": "fka/awesome-chatgpt-prompts-10", "主題": "ansewr and Questions", "来源": "Peneolice", "回复": [{"楼ID": "0", "回复": " I have some Q about a text. into the universe of Technical images", "扩展字段": {"回复人": "Peneolice", "引用内容": "", "回复时间": "2023-06-17T15:18:27"}}], "元数据": {"href": "/datasets/fka/awesome-chatgpt-prompts/discussions/10"}}, {"ID": "fka/awesome-chatgpt-prompts-7", "主題": "exemple", "来源": "florymignon", "回复": [{"楼ID": "0", "回复": "", "扩展字段": {"回复人": "florymignon", "引用内容": "", "回复时间": "2023-04-16T17:12:49"}}], "元数据": {"href": "/datasets/fka/awesome-chatgpt-prompts/discussions/7"}}, {"ID": "fka/awesome-chatgpt-prompts-5", "主題": "Interesting repo", "来源": "marcosfp", "回复": [{"楼ID": "0", "回复": "First of all, apologies in advance if this is not the appropriate forum to do this. First of all, congrats Fka on this amazing dataset. I have developed a tool that tries to \"hide\" the context from end-users wrapping them into the petition. This is interesting since end-users would not provide a complex prompt: \nhttps://github.com/citiususc/Smarty-GPT\nFeel free to take a look, any comments are deeply appreciated and sorry again if this is not the appropriate channel.", "扩展字段": {"回复人": "marcosfp", "引用内容": "", "回复时间": "2023-03-15T20:46:47"}}], "元数据": {"href": "/datasets/fka/awesome-chatgpt-prompts/discussions/5"}}, {"ID": "fka/awesome-chatgpt-prompts-4", "主題": "Explore general artificial intelligence AGI ability of chatGPT ？", "来源": "hayooucom", "回复": [{"楼ID": "0", "回复": "I am an AI enthusiast and often write code. I also worked on exploring AGI 5 years ago. With the popularity of chatGPT, I found that it can realize the small system of AGI, which is part of my work:\nmy workhttps://github.com/youkpan/AGI-ChatGPT\nChatGPT doing complex mission:\nWe found that chatGPT's AI can do many things, so we wrote a small contextual operating system to explore the AGI field of chatGPT and the real world with the help of the server's capabilities (networking, storage, sending information, generating voice, pictures, picture recognition, etc.) communication skills\n\nThe prompt starts:\nYou need to reply in this format:\n....lot , and i proved It's work !", "扩展字段": {"回复人": "hayooucom", "引用内容": "", "回复时间": "2023-02-24T02:44:18"}}], "元数据": {"href": "/datasets/fka/awesome-chatgpt-prompts/discussions/4"}}, {"ID": "fka/awesome-chatgpt-prompts-48", "主題": "Request: DOI", "来源": "Amyww", "回复": [{"楼ID": "0", "回复": "？", "扩展字段": {"回复人": "Amyww", "引用内容": "", "回复时间": "2024-09-13T03:36:45"}}], "元数据": {"href": "/datasets/fka/awesome-chatgpt-prompts/discussions/48"}}, {"ID": "fka/awesome-chatgpt-prompts-39", "主題": "Best ChatGPT assistant?", "来源": "Matthewpan1", "回复": [{"楼ID": "0", "回复": "I have been working  on this assistant for a few days now and it is so much better than the most popular ChatGPT assistant available with 5.5k interactions.https://hf.co/chat/assistant/6687e1370f68811f5dd940f5", "扩展字段": {"回复人": "Matthewpan1", "引用内容": "", "回复时间": "2024-07-19T13:43:27"}}, {"楼ID": "1", "回复": "Are you still using this", "扩展字段": {"回复人": "Ekimnedops6969", "引用内容": "", "回复时间": "2024-08-10T08:39:29"}}], "元数据": {"href": "/datasets/fka/awesome-chatgpt-prompts/discussions/39"}}, {"ID": "fka/awesome-chatgpt-prompts-29", "主題": "asd", "来源": "phatle157", "回复": [{"楼ID": "0", "回复": "dsada", "扩展字段": {"回复人": "phatle157", "引用内容": "", "回复时间": "2024-01-17T16:46:38"}}], "元数据": {"href": "/datasets/fka/awesome-chatgpt-prompts/discussions/29"}}, {"ID": "fka/awesome-chatgpt-prompts-27", "主題": "#27", "来源": "phantuecs", "回复": [{"楼ID": "0", "回复": "", "扩展字段": {"回复人": "phantuecs", "引用内容": "", "回复时间": "2023-12-22T02:56:46"}}, {"楼ID": "1", "回复": "", "扩展字段": {"回复人": "phantuecs", "引用内容": "", "回复时间": "2023-12-22T03:01:19"}}], "元数据": {"href": "/datasets/fka/awesome-chatgpt-prompts/discussions/27"}}]
[{"ID": "neuralwork/arxiver-2", "主題": "Code for Processing Papers", "来源": "mixeden", "回复": [{"楼ID": "0", "回复": "Hey there guys, could you please open-source code for processing papers / dataset preparation?", "扩展字段": {"回复人": "mixeden", "引用内容": "", "回复时间": "2024-10-20T19:27:01"}}, {"楼ID": "1", "回复": "\n@mixeden\n you can find the code in this repo:https://github.com/neuralwork/arxiver", "扩展字段": {"回复人": "adirik", "引用内容": "", "回复时间": "2024-11-01T21:16:34"}}], "元数据": {"href": "/datasets/neuralwork/arxiver/discussions/2"}}, {"ID": "neuralwork/arxiver-1", "主題": "[bot] Conversion to Parquet", "来源": "parquet-converter", "回复": [{"楼ID": "0", "回复": "The parquet-converter bot has created a version of this dataset in the Parquet format in the refs/convert/parquet branch.\nApache Parquet is a popular columnar storage format known for:\nThis is what powers the dataset viewer on each dataset page and every dataset on the Hub can be accessed with the same code (you can use HF Datasets, ClickHouse, DuckDB, Pandas or Polars, up to you).\nYou can learn more about the advantages associated with Parquet in the documentation.\nYou can access the Parquet version of the dataset by following this link: refs/convert/parquet\nWhen the dataset is already in Parquet format, the data are not converted and the files in refs/convert/parquet are links to the original files. This rule has an exception to ensure the dataset viewer API to stay fast: if the row group size of the original Parquet files is too big, new Parquet files are generated.\nYou don't need to do anything. The Parquet version of the dataset is available for you to use. Refer to the documentation for examples and code snippets on how to query the Parquet files with ClickHouse, DuckDB, Pandas or Polars.\nIf you have any questions or concerns, feel free to ask in the discussion below. You can also close the discussion if you don't have any questions.", "扩展字段": {"回复人": "parquet-converter", "引用内容": "", "回复时间": "2024-10-20T00:10:25"}}], "元数据": {"href": "/datasets/neuralwork/arxiver/discussions/1"}}]
[{"ID": "Spawning/PD12M-2", "主題": "What are the potential Applications?", "来源": "shravankumar147", "回复": [{"楼ID": "0", "回复": "What are some potential applications we can build using this dataset? ", "扩展字段": {"回复人": "shravankumar147", "引用内容": "", "回复时间": "2024-11-02T18:03:12"}}], "元数据": {"href": "/datasets/Spawning/PD12M/discussions/2"}}, {"ID": "Spawning/PD12M-1", "主題": "[bot] Conversion to Parquet", "来源": "parquet-converter", "回复": [{"楼ID": "0", "回复": "The parquet-converter bot has created a version of this dataset in the Parquet format in the refs/convert/parquet branch.\nApache Parquet is a popular columnar storage format known for:\nThis is what powers the dataset viewer on each dataset page and every dataset on the Hub can be accessed with the same code (you can use HF Datasets, ClickHouse, DuckDB, Pandas or Polars, up to you).\nYou can learn more about the advantages associated with Parquet in the documentation.\nYou can access the Parquet version of the dataset by following this link: refs/convert/parquet\nWhen the dataset is already in Parquet format, the data are not converted and the files in refs/convert/parquet are links to the original files. This rule has an exception to ensure the dataset viewer API to stay fast: if the row group size of the original Parquet files is too big, new Parquet files are generated.\nYou don't need to do anything. The Parquet version of the dataset is available for you to use. Refer to the documentation for examples and code snippets on how to query the Parquet files with ClickHouse, DuckDB, Pandas or Polars.\nIf you have any questions or concerns, feel free to ask in the discussion below. You can also close the discussion if you don't have any questions.", "扩展字段": {"回复人": "parquet-converter", "引用内容": "", "回复时间": "2024-09-27T00:10:37"}}], "元数据": {"href": "/datasets/Spawning/PD12M/discussions/1"}}]
[{"ID": "vikhyatk/lofi-3", "主題": "[bot] Conversion to Parquet", "来源": "parquet-converter", "回复": [{"楼ID": "0", "回复": "The parquet-converter bot has created a version of this dataset in the Parquet format in the refs/convert/parquet branch.\nApache Parquet is a popular columnar storage format known for:\nThis is what powers the dataset viewer on each dataset page and every dataset on the Hub can be accessed with the same code (you can use HF Datasets, ClickHouse, DuckDB, Pandas or Polars, up to you).\nYou can learn more about the advantages associated with Parquet in the documentation.\nYou can access the Parquet version of the dataset by following this link: refs/convert/parquet\nWhen the dataset is already in Parquet format, the data are not converted and the files in refs/convert/parquet are links to the original files. This rule has an exception to ensure the dataset viewer API to stay fast: if the row group size of the original Parquet files is too big, new Parquet files are generated.\nYou don't need to do anything. The Parquet version of the dataset is available for you to use. Refer to the documentation for examples and code snippets on how to query the Parquet files with ClickHouse, DuckDB, Pandas or Polars.\nIf you have any questions or concerns, feel free to ask in the discussion below. You can also close the discussion if you don't have any questions.", "扩展字段": {"回复人": "parquet-converter", "引用内容": "", "回复时间": "2024-07-28T00:10:28"}}], "元数据": {"href": "/datasets/vikhyatk/lofi/discussions/3"}}]
[{"ID": "BAAI/Infinity-MM-1", "主題": "坐标尺度", "来源": "paul91", "回复": [{"楼ID": "0", "回复": "想问下数据中有跟坐标相关的信息，都是用0～1小数表示的，如果要可视化这些坐标，是只需要乘以实际的宽、高就可以还原吗？", "扩展字段": {"回复人": "paul91", "引用内容": "", "回复时间": "2024-11-01T16:02:06"}}], "元数据": {"href": "/datasets/BAAI/Infinity-MM/discussions/1"}}]
[{"ID": "GAIR/o1-journey-2", "主題": "How to divide into steps", "来源": "sxcasf", "回复": [{"楼ID": "0", "回复": "How to divide into steps", "扩展字段": {"回复人": "sxcasf", "引用内容": "", "回复时间": "2024-10-18T08:53:26"}}], "元数据": {"href": "/datasets/GAIR/o1-journey/discussions/2"}}, {"ID": "GAIR/o1-journey-1", "主題": "[bot] Conversion to Parquet", "来源": "parquet-converter", "回复": [{"楼ID": "0", "回复": "The parquet-converter bot has created a version of this dataset in the Parquet format in the refs/convert/parquet branch.\nApache Parquet is a popular columnar storage format known for:\nThis is what powers the dataset viewer on each dataset page and every dataset on the Hub can be accessed with the same code (you can use HF Datasets, ClickHouse, DuckDB, Pandas or Polars, up to you).\nYou can learn more about the advantages associated with Parquet in the documentation.\nYou can access the Parquet version of the dataset by following this link: refs/convert/parquet\nWhen the dataset is already in Parquet format, the data are not converted and the files in refs/convert/parquet are links to the original files. This rule has an exception to ensure the dataset viewer API to stay fast: if the row group size of the original Parquet files is too big, new Parquet files are generated.\nYou don't need to do anything. The Parquet version of the dataset is available for you to use. Refer to the documentation for examples and code snippets on how to query the Parquet files with ClickHouse, DuckDB, Pandas or Polars.\nIf you have any questions or concerns, feel free to ask in the discussion below. You can also close the discussion if you don't have any questions.", "扩展字段": {"回复人": "parquet-converter", "引用内容": "", "回复时间": "2024-10-17T00:10:45"}}], "元数据": {"href": "/datasets/GAIR/o1-journey/discussions/1"}}]
[{"ID": "marcelbinz/Psych-101-1", "主題": "[bot] Conversion to Parquet", "来源": "parquet-converter", "回复": [{"楼ID": "0", "回复": "The parquet-converter bot has created a version of this dataset in the Parquet format in the refs/convert/parquet branch.\nApache Parquet is a popular columnar storage format known for:\nThis is what powers the dataset viewer on each dataset page and every dataset on the Hub can be accessed with the same code (you can use HF Datasets, ClickHouse, DuckDB, Pandas or Polars, up to you).\nYou can learn more about the advantages associated with Parquet in the documentation.\nYou can access the Parquet version of the dataset by following this link: refs/convert/parquet\nWhen the dataset is already in Parquet format, the data are not converted and the files in refs/convert/parquet are links to the original files. This rule has an exception to ensure the dataset viewer API to stay fast: if the row group size of the original Parquet files is too big, new Parquet files are generated.\nYou don't need to do anything. The Parquet version of the dataset is available for you to use. Refer to the documentation for examples and code snippets on how to query the Parquet files with ClickHouse, DuckDB, Pandas or Polars.\nIf you have any questions or concerns, feel free to ask in the discussion below. You can also close the discussion if you don't have any questions.", "扩展字段": {"回复人": "parquet-converter", "引用内容": "", "回复时间": "2024-10-27T00:10:13"}}], "元数据": {"href": "/datasets/marcelbinz/Psych-101/discussions/1"}}]
[{"ID": "amphion/Emilia-Dataset-7", "主題": "data error", "来源": "laixiaofeng", "回复": [{"楼ID": "0", "回复": "There are many Cantonese pronunciations in it, and the corresponding text is wrong. Due to the large amount of data, it has a great impact on our use.example:ZH_B00042_S05291_W000054ZH_B00042_S05291_W000055ZH_B00041_S04659_W000036", "扩展字段": {"回复人": "laixiaofeng", "引用内容": "", "回复时间": "2024-10-21T00:45:41"}}, {"楼ID": "1", "回复": "Hi,We use the whisper medium model to generate the transcript. However, the medium model did not support the yue language (check it here) which might have caused the wrong text.\nTo fix this, you can use a model like whisper-large-v3 or whisper-large-v3-turbo (which supports yue language) to run detect_language again, then:\nI think these steps can help filter out those \"bad\" audio.\nA sample:", "扩展字段": {"回复人": "yuantuo666", "引用内容": "", "回复时间": "2024-10-21T11:02:10"}}], "元数据": {"href": "/datasets/amphion/Emilia-Dataset/discussions/7"}}, {"ID": "amphion/Emilia-Dataset-6", "主題": "Synthetic speech", "来源": "cdminix", "回复": [{"楼ID": "0", "回复": "Quite a few of the English samples are synthetic speech, does anyone know a good way to filter those out?", "扩展字段": {"回复人": "cdminix", "引用内容": "", "回复时间": "2024-10-17T14:23:12"}}, {"楼ID": "1", "回复": "Hi thanks for the question, maybe this scripts help https://github.com/SWivid/F5-TTS/blob/main/scripts/prepare_emilia.py.", "扩展字段": {"回复人": "HarryHe", "引用内容": "", "回复时间": "2024-10-18T07:44:27"}}, {"楼ID": "2", "回复": "Were the specific synthetic file IDs in the prepare script excluded from the dataset?In my sample of 200 clips almost 20% were likely synthetic...", "扩展字段": {"回复人": "cdminix", "引用内容": "", "回复时间": "2024-10-18T09:27:39"}}, {"楼ID": "3", "回复": "This script excludes certain audio files that appear to be synthesized or heavily code-switched, which could still be relevant for your use case. You may refer to our source audio list at this link (https://huggingface.co/datasets/amphion/Emilia), where tags indicating whether an audio file is synthesized are available. For more robust filtering, it is recommended to train a state-of-the-art (SOTA) anti-spoofing model, such as the one available here (https://github.com/TakHemlata/SSL_Anti-spoofing), which provides a pre-trained checkpoint (ckpt). You can then use this model to infer over the entire dataset and filter out audio files that are confidently identified as synthesized.", "扩展字段": {"回复人": "HarryHe", "引用内容": "", "回复时间": "2024-10-19T08:30:42"}}, {"楼ID": "4", "回复": "Thank you very much, that resolves it for me :)", "扩展字段": {"回复人": "cdminix", "引用内容": "", "回复时间": "2024-10-21T06:58:02"}}], "元数据": {"href": "/datasets/amphion/Emilia-Dataset/discussions/6"}}, {"ID": "amphion/Emilia-Dataset-5", "主題": "Missing metadata & multilingual problems", "来源": "eminentgu", "回复": [{"楼ID": "0", "回复": "First I would love to thank you for ur solid work and contributions.When I dealing with the dataset, I ran into some small issues:", "扩展字段": {"回复人": "eminentgu", "引用内容": "好的，我今天已经可以开始健身了吗，我今天就开始吗 ？Ja klar, zuerst machen wir einen Gesundheitscheck und besprechen, was du machen möchtest.\n11.817( duration )  3.0946 (dnsmos)  Ja klar, zuerst machen wir einen Gesundheitscheck und besprechen, was du machen möchtest.", "回复时间": "2024-09-06T08:32:05"}}, {"楼ID": "1", "回复": "Hi, thanks for your usage and feedback. Here is some response to your comments:", "扩展字段": {"回复人": "yuantuo666", "引用内容": "", "回复时间": "2024-09-06T13:05:29"}}], "元数据": {"href": "/datasets/amphion/Emilia-Dataset/discussions/5"}}, {"ID": "amphion/Emilia-Dataset-3", "主題": "Using language as a basis for spliting datasets", "来源": "julyxia", "回复": [{"楼ID": "0", "回复": "Can you divide the dataset by language? Similar to https://huggingface.co/datasets/facebook/voxpopuli . In fact, we prefer to download minority languages.", "扩展字段": {"回复人": "julyxia", "引用内容": "", "回复时间": "2024-09-02T02:00:11"}}, {"楼ID": "1", "回复": "Thank you for your attention to Emilia, actually we have divided the dataset by languages. Click \"Files and versions\" and you may have subfolders for each languages including, EN, ZH, DE, FR, JP, KO", "扩展字段": {"回复人": "HarryHe", "引用内容": "", "回复时间": "2024-09-02T03:45:53"}}, {"楼ID": "2", "回复": "Thanks for your reply. I hope to run load_dataset(\"amphion/Emilia-Dataset\", languages=['de']) and download the de data instead of the full data by specifying the LID in the languages ​​parameter.  ", "扩展字段": {"回复人": "julyxia", "引用内容": "", "回复时间": "2024-09-02T07:53:54"}}, {"楼ID": "3", "回复": "Thanks for your suggestion. I think you can use this feature as introduced in HuggingFace docs to load specific language data.\nE.g.\nWe are planning to add this to our README.md. Please let us know if this works :)", "扩展字段": {"回复人": "yuantuo666", "引用内容": "", "回复时间": "2024-09-02T14:27:35"}}, {"楼ID": "4", "回复": "This is my test; it looks like it is working since we do have 90 DE tar files.\n", "扩展字段": {"回复人": "yuantuo666", "引用内容": "", "回复时间": "2024-09-02T14:37:27"}}, {"楼ID": "5", "回复": "Thank you very much and best wish to you", "扩展字段": {"回复人": "julyxia", "引用内容": "", "回复时间": "2024-09-03T01:39:12"}}], "元数据": {"href": "/datasets/amphion/Emilia-Dataset/discussions/3"}}, {"ID": "amphion/Emilia-Dataset-2", "主題": "[bot] Conversion to Parquet", "来源": "parquet-converter", "回复": [{"楼ID": "0", "回复": "The parquet-converter bot has created a version of this dataset in the Parquet format in the refs/convert/parquet branch.\nApache Parquet is a popular columnar storage format known for:\nThis is what powers the dataset viewer on each dataset page and every dataset on the Hub can be accessed with the same code (you can use HF Datasets, ClickHouse, DuckDB, Pandas or Polars, up to you).\nYou can learn more about the advantages associated with Parquet in the documentation.\nYou can access the Parquet version of the dataset by following this link: refs/convert/parquet\nWhen the dataset is already in Parquet format, the data are not converted and the files in refs/convert/parquet are links to the original files. This rule has an exception to ensure the dataset viewer API to stay fast: if the row group size of the original Parquet files is too big, new Parquet files are generated.\nYou don't need to do anything. The Parquet version of the dataset is available for you to use. Refer to the documentation for examples and code snippets on how to query the Parquet files with ClickHouse, DuckDB, Pandas or Polars.\nIf you have any questions or concerns, feel free to ask in the discussion below. You can also close the discussion if you don't have any questions.", "扩展字段": {"回复人": "parquet-converter", "引用内容": "", "回复时间": "2024-09-02T00:10:16"}}], "元数据": {"href": "/datasets/amphion/Emilia-Dataset/discussions/2"}}]
[{"ID": "BAAI/Infinity-Instruct-24", "主題": "7M数据缺少CodeFeedback-Filtered-Instruction组成", "来源": "xielipeng", "回复": [{"楼ID": "0", "回复": "如上", "扩展字段": {"回复人": "xielipeng", "引用内容": "", "回复时间": "2024-10-23T08:45:40"}}], "元数据": {"href": "/datasets/BAAI/Infinity-Instruct/discussions/24"}}, {"ID": "BAAI/Infinity-Instruct-23", "主題": "What is the Differences and Overlap between 7M, 7M Domain, Gen and 0625?", "来源": "alpayariyak", "回复": [{"楼ID": "0", "回复": "Hi,Could you share a bit more details, differences and commonalities between these sets?", "扩展字段": {"回复人": "alpayariyak", "引用内容": "", "回复时间": "2024-09-04T18:24:47"}}, {"楼ID": "1", "回复": "", "扩展字段": {"回复人": "hyxmmm", "引用内容": "", "回复时间": "2024-09-06T02:04:22"}}, {"楼ID": "2", "回复": "To understand the details of the different data versions, you can refer to this chart.", "扩展字段": {"回复人": "hyxmmm", "引用内容": "", "回复时间": "2024-09-06T02:15:29"}}], "元数据": {"href": "/datasets/BAAI/Infinity-Instruct/discussions/23"}}, {"ID": "BAAI/Infinity-Instruct-22", "主題": "About `system_prompt` setting when fine-tuning by this dataset", "来源": "Remixa", "回复": [{"楼ID": "0", "回复": "Thank you for your contributions!\nI am a novice and would like to ask how do you set the 'system_prompt' when using these datasets for fine-tuning? Don't set it? Or set only one kind of 'system_prompt'?", "扩展字段": {"回复人": "Remixa", "引用内容": "", "回复时间": "2024-09-04T09:54:43"}}, {"楼ID": "1", "回复": "Hello, we tried fine tuning with system prompt didn't help. However, after fine-tuning, if you need to use the model for certain tasks downstream, such as roleplaying, you can add a custom system prompt to further enhance the model.", "扩展字段": {"回复人": "hyxmmm", "引用内容": "", "回复时间": "2024-09-06T02:20:26"}}, {"楼ID": "2", "回复": "\n@hyxmmm\n I get it, thanks a lot!", "扩展字段": {"回复人": "Remixa", "引用内容": "", "回复时间": "2024-09-06T02:25:37"}}], "元数据": {"href": "/datasets/BAAI/Infinity-Instruct/discussions/22"}}, {"ID": "BAAI/Infinity-Instruct-21", "主題": "git lfs pull 失败", "来源": "zheong", "回复": [{"楼ID": "0", "回复": "git lfs pullbatch response: Authentication required: Access to dataset BAAI/Infinity-Instruct is restricted. You must be authenticated to access it.batch response: Authentication required: Access to dataset BAAI/Infinity-Instruct is restricted. You must be authenticated to access it.batch response: Authentication required: Access to dataset BAAI/Infinity-Instruct is restricted. You must be authenticated to access it.error: failed to fetch some objects from 'https://huggingface.co/datasets/BAAI/Infinity-Instruct.git/info/lfs'\n这个什么情况，怎么解决", "扩展字段": {"回复人": "zheong", "引用内容": "", "回复时间": "2024-09-03T04:06:22"}}, {"楼ID": "1", "回复": "可以参考这里的类似问题：https://discuss.huggingface.co/t/problem-with-lfs-pull/68867建议使用huggingface-cli download作为替代下载数据集，使用前需要先用huggingface的access token登陆。这里有详细的使用说明：https://huggingface.co/docs/huggingface_hub/guides/cli#huggingface-cli-download", "扩展字段": {"回复人": "hyxmmm", "引用内容": "", "回复时间": "2024-09-03T06:18:13"}}], "元数据": {"href": "/datasets/BAAI/Infinity-Instruct/discussions/21"}}, {"ID": "BAAI/Infinity-Instruct-20", "主題": "Decontamination?", "来源": "alpayariyak", "回复": [{"楼ID": "0", "回复": "Hi, thank you for your great work!\nYour dataset selection is great. I was wondering the following - have you done any decontamination? Datasets like Code Alpaca , and as a result its derivatives, are quite contaminated, this is from LMSys LLM-Decontaminator Repo:\nHas there been any decontamination work? For both exact match and rephrased samples.", "扩展字段": {"回复人": "alpayariyak", "引用内容": "", "回复时间": "2024-08-28T19:46:23"}}, {"楼ID": "1", "回复": "Thanks so much for your thoughtful feedback! I want to assure you that we've taken data contamination very seriously, with thorough checks in place to prevent it. We'll be sharing all the details of our methods and findings in the upcoming technical documentation.\nTo further reduce the risk of contamination, we’ve switched to a new benchmark, open-llm-leaderboard2, for evaluations. The results have been really promising—our fine-tuned models have climbed to the top of the LLM leaderboard2(Mixtral-7B based models), which mainly uses fresh evaluation sets. This really shows how strong our dataset is.\n\nWe do appreciate the alternative approach you mentioned, but it has some significant downsides, including high costs. In our view, the best way to tackle data contamination is through careful evaluation. Reducing data diversity might actually hurt the model’s performance, which we definitely want to avoid.", "扩展字段": {"回复人": "ZacLiu", "引用内容": "", "回复时间": "2024-08-29T03:58:15"}}], "元数据": {"href": "/datasets/BAAI/Infinity-Instruct/discussions/20"}}, {"ID": "BAAI/Infinity-Instruct-19", "主題": "指令数据集的类别映射问题", "来源": "Alwin114", "回复": [{"楼ID": "0", "回复": "数据集中的标签和统计图中的标签是怎么样的对应关系？\n", "扩展字段": {"回复人": "Alwin114", "引用内容": "", "回复时间": "2024-08-27T02:56:53"}}, {"楼ID": "1", "回复": "图里的标签主要来自于cate_ability_en部分。为了画图美观对部分标签做了简写。", "扩展字段": {"回复人": "hyxmmm", "引用内容": "", "回复时间": "2024-08-30T06:48:31"}}], "元数据": {"href": "/datasets/BAAI/Infinity-Instruct/discussions/19"}}, {"ID": "BAAI/Infinity-Instruct-18", "主題": "3M 和 7M 的中文数据是相同的吗？", "来源": "xianf", "回复": [{"楼ID": "0", "回复": "我抽取出来发现中文部分都只有 700k 的数据。", "扩展字段": {"回复人": "xianf", "引用内容": "", "回复时间": "2024-08-21T03:11:43"}}, {"楼ID": "1", "回复": "是的，已发布的版本以英文为主。目前开源高质量的中文指令较少，我们还在做筛选和合成。如果是在中英双语模型上做继续微调，可以先试试7M和0729。双语模型有一定的语言迁移能力，理论上英文能力的提升也能带动中文对应能力的提升。", "扩展字段": {"回复人": "hyxmmm", "引用内容": "", "回复时间": "2024-08-25T14:14:36"}}], "元数据": {"href": "/datasets/BAAI/Infinity-Instruct/discussions/18"}}, {"ID": "BAAI/Infinity-Instruct-17", "主題": "the perspective of instruction compliance", "来源": "YvanLee", "回复": [{"楼ID": "0", "回复": "Validate the evolved data, and use AI assistants to eliminate data that failed to evolve from the perspective of instruction compliance请问这句话如何理解", "扩展字段": {"回复人": "YvanLee", "引用内容": "", "回复时间": "2024-08-19T06:17:29"}}, {"楼ID": "1", "回复": "数据进化有一定的不确定性，生成的指令可能质量较差或是与原本的种子指令包含的信息一致。详情可以参考一下WizardLM (https://arxiv.org/pdf/2304.12244) 原文对于指令进化失败的定义：", "扩展字段": {"回复人": "hyxmmm", "引用内容": "", "回复时间": "2024-08-20T09:15:51"}}], "元数据": {"href": "/datasets/BAAI/Infinity-Instruct/discussions/17"}}, {"ID": "BAAI/Infinity-Instruct-16", "主題": "0729聊天数据集有计划开源吗？", "来源": "yixinsong", "回复": [{"楼ID": "0", "回复": "非常棒的工作！", "扩展字段": {"回复人": "yixinsong", "引用内容": "", "回复时间": "2024-08-07T07:55:35"}}, {"楼ID": "1", "回复": "感谢关注！我们在做最后的数据污染检查，计划会在一周内发布。", "扩展字段": {"回复人": "hyxmmm", "引用内容": "", "回复时间": "2024-08-08T02:56:41"}}, {"楼ID": "2", "回复": "0729已经发布：", "扩展字段": {"回复人": "ZacLiu", "引用内容": "", "回复时间": "2024-08-17T05:30:06"}}], "元数据": {"href": "/datasets/BAAI/Infinity-Instruct/discussions/16"}}, {"ID": "BAAI/Infinity-Instruct-15", "主題": "0625 Split Error: `pyarrow.lib.ArrowInvalid: Expected to read 538970747 metadata bytes, but only read 1072`", "来源": "Avelina", "回复": [{"楼ID": "0", "回复": "Like the title says, we get the following error for the 0625 split: pyarrow.lib.ArrowInvalid: Expected to read 538970747 metadata bytes, but only read 1072Which can be fixed temporarily using revision='3470c71f70ea906d2bfe13f9131d99ca92ba6da7' until this gets sorted.\nSimilar issue with the 7M domains split, but no prior revision to use.", "扩展字段": {"回复人": "Avelina", "引用内容": "", "回复时间": "2024-08-06T05:00:59"}}, {"楼ID": "1", "回复": "same problem", "扩展字段": {"回复人": "Spurslipu", "引用内容": "", "回复时间": "2024-08-06T07:42:28"}}, {"楼ID": "2", "回复": "It seems that the arrow dataset is not supported to be loaded with load_dataset(). We re-uploaded the dataset in parquet format. Please download it again. Sorry for the trouble.", "扩展字段": {"回复人": "hyxmmm", "引用内容": "", "回复时间": "2024-08-06T14:59:42"}}], "元数据": {"href": "/datasets/BAAI/Infinity-Instruct/discussions/15"}}, {"ID": "BAAI/Infinity-Instruct-14", "主題": "关于8月新更新的数据集问题", "来源": "Spurslipu", "回复": [{"楼ID": "0", "回复": "1、7M的数据是可以完全替换3M的数据吗？2、7M数据和7Mdomain数据是完全同一数据源的嘛？也就是说7M domain数据相当于是7M的任务划分版本", "扩展字段": {"回复人": "Spurslipu", "引用内容": "", "回复时间": "2024-08-05T10:19:13"}}, {"楼ID": "1", "回复": "", "扩展字段": {"回复人": "hyxmmm", "引用内容": "", "回复时间": "2024-08-06T15:00:24"}}, {"楼ID": "2", "回复": "请问下7M数据集的response是gpt4 还是用什么模型的？", "扩展字段": {"回复人": "unoc", "引用内容": "", "回复时间": "2024-08-14T03:18:29"}}, {"楼ID": "3", "回复": "7M是从多个开源数据集中挑选的。这些数据集可能由人、gpt4或者其他模型产出，可以在Huggingface上查看有关这些数据集制作的详细介绍：\n", "扩展字段": {"回复人": "hyxmmm", "引用内容": "", "回复时间": "2024-08-14T06:21:23"}}, {"楼ID": "4", "回复": "Infinity-Instruct中标注了每条数据的来源数据集，可以根据标注筛选仅由gpt4/人/其他模型产出的数据。", "扩展字段": {"回复人": "hyxmmm", "引用内容": "", "回复时间": "2024-08-14T06:24:10"}}, {"楼ID": "5", "回复": "请教一下gen数据和chat数据有啥区别呢", "扩展字段": {"回复人": "Spurslipu", "引用内容": "", "回复时间": "2024-08-21T03:57:38"}}], "元数据": {"href": "/datasets/BAAI/Infinity-Instruct/discussions/14"}}, {"ID": "BAAI/Infinity-Instruct-11", "主題": "部分对话开头应该是来自系统", "来源": "VIPSP", "回复": [{"楼ID": "0", "回复": "您好，数据集中有一些对话，比如{'id': 2021963, 'conversations': [{'from': 'gpt', 'value': 'You are an AI assistant. You will be given a task. You must generate a detailed and long answer.'}, {'from': 'human', 'value': 'Dr. Jones stud...............}]}，这里第一句话似乎应该是来自system，而不是gpt，在这些对话中，人类的回答有些是以“\\nAnswer:”之类作为结尾，也有点奇怪，之前的版本好像没有这些问题。期待您的答复，感谢您辛苦的工作！", "扩展字段": {"回复人": "VIPSP", "引用内容": "", "回复时间": "2024-07-11T08:30:41"}}, {"楼ID": "1", "回复": "感谢提醒！我们确实发现部分来源数据集会将system角色统一用gpt标识，我们会尽快修改避免混淆。您也可以先在训练时用处理system的方式处理第一轮角色为gpt的对话。", "扩展字段": {"回复人": "hyxmmm", "引用内容": "您好，数据集中有一些对话，比如{'id': 2021963, 'conversations': [{'from': 'gpt', 'value': 'You are an AI assistant. You will be given a task. You must generate a detailed and long answer.'}, {'from': 'human', 'value': 'Dr. Jones stud...............}]}，这里第一句话似乎应该是来自system，而不是gpt，在这些对话中，人类的回答有些是以“\\nAnswer:”之类作为结尾，也有点奇怪，之前的版本好像没有这些问题。期待您的答复，感谢您辛苦的工作！", "回复时间": "2024-07-12T03:55:16"}}], "元数据": {"href": "/datasets/BAAI/Infinity-Instruct/discussions/11"}}, {"ID": "BAAI/Infinity-Instruct-10", "主題": "关于3M数据和chat数据的使用", "来源": "Spurslipu", "回复": [{"楼ID": "0", "回复": "想请教一下3M数据和chat数据的使用有什么推荐的方法吗？我这边能想到的大概有以下几种1、stage1 3M，stage2 chat数据2、3M数据和chat数据混在一起，但是我不清楚应该是用怎么样的数据比例3、只使用chat数据\n期待你们的回复，感谢", "扩展字段": {"回复人": "Spurslipu", "引用内容": "", "回复时间": "2024-07-10T09:50:00"}}, {"楼ID": "1", "回复": "因为chat的数据量相比3M较少，所以推荐是使用stage 1 3M + stage 2 chat。", "扩展字段": {"回复人": "hyxmmm", "引用内容": "", "回复时间": "2024-07-10T10:02:40"}}, {"楼ID": "2", "回复": "okok 感谢，我先试试，有问题了再交流", "扩展字段": {"回复人": "Spurslipu", "引用内容": "", "回复时间": "2024-07-10T10:04:11"}}, {"楼ID": "3", "回复": "哦对，0625这个版本3M数据相比之前有更新嘛？我需要替换成最新的3M数据嘛", "扩展字段": {"回复人": "Spurslipu", "引用内容": "", "回复时间": "2024-07-10T10:06:31"}}, {"楼ID": "4", "回复": "3M没有更新，用之前那版就可以", "扩展字段": {"回复人": "hyxmmm", "引用内容": "", "回复时间": "2024-07-10T10:08:53"}}, {"楼ID": "5", "回复": "hi, 我看推荐是使用两个 stage 训练，但是公开的只有一组参数，所以想请问一下具体的细节： https://huggingface.co/BAAI/Infinity-Instruct-3M-0625-Qwen2-7B/discussions/3  两个stage 的训练都用一组参数吗？", "扩展字段": {"回复人": "bbruceyuan", "引用内容": "因为chat的数据量相比3M较少，所以推荐是使用stage 1 3M + stage 2 chat。", "回复时间": "2024-09-02T08:46:39"}}, {"楼ID": "6", "回复": "两个stage的参数可以用一样的。只是用不同预训练模型作为起点需要调一下lr，保证SFT期间loss不上涨。比如这个问题里以Qwen2作为起点训练的模型，他的两阶段lr都是1e-5。而以mistral作为起点训练的模型lr需要调整到5e-6。", "扩展字段": {"回复人": "hyxmmm", "引用内容": "", "回复时间": "2024-09-03T06:03:37"}}], "元数据": {"href": "/datasets/BAAI/Infinity-Instruct/discussions/10"}}, {"ID": "BAAI/Infinity-Instruct-5", "主題": "Language field", "来源": "Tijmen2", "回复": [{"楼ID": "0", "回复": "Looks helpful, thanks. Can you include a field for the language of each entry?", "扩展字段": {"回复人": "Tijmen2", "引用内容": "", "回复时间": "2024-06-26T00:37:14"}}, {"楼ID": "1", "回复": "Sure! We'll be updating the next version to include the language of each entry, along with various categories and rating results. Thank you for your attention and support!", "扩展字段": {"回复人": "ZacLiu", "引用内容": "", "回复时间": "2024-06-26T06:01:03"}}], "元数据": {"href": "/datasets/BAAI/Infinity-Instruct/discussions/5"}}, {"ID": "BAAI/Infinity-Instruct-4", "主題": "Different dataset versions ? 3M / 0608 / 06012", "来源": "philschmid", "回复": [{"楼ID": "0", "回复": "Hello, \nThank you for the great work! I was looking at the tests you ran and couldn't find any information on the difference between InfInstruct-3M and *-0608 or *-0612 version. Can you share what the difference here are? \nInfInstruct-3M-Mistral-7B\t7.3\t14.3InfInstruct-Mistral-7B 0608\t7.8\t16.9InfInstruct-Mistral-7B 0612 7.9\t25.1", "扩展字段": {"回复人": "philschmid", "引用内容": "", "回复时间": "2024-06-20T06:39:43"}}, {"楼ID": "1", "回复": "The InfInstruct-3M is a diverse collection of instructions curated from multiple open-source datasets, designed to enhance foundational capabilities. Additionally, the 0608 and 0612 sets comprise generated instructions to improve conversational abilities.\nSo, we perform a two-stage supervised fine-tuning (SFT) on Mistral-7B. The first stage utilizes the InfInstruct-3M dataset to enhance foundational capabilities, while the second stage employs the 0612 dataset to improve conversational abilities.\nThe final version will consist of two parts: InfInstruct-8M and InfInstruct-conv-2M.", "扩展字段": {"回复人": "ZacLiu", "引用内容": "", "回复时间": "2024-06-20T07:46:41"}}, {"楼ID": "2", "回复": "Looking forward to this! ", "扩展字段": {"回复人": "philschmid", "引用内容": "The final version will consist of two parts: InfInstruct-8M and InfInstruct-conv-2M.", "回复时间": "2024-06-20T11:39:06"}}, {"楼ID": "3", "回复": "two-stage supervised fine-tuning (SFT) on Mistral-7B, Do both stages of training only calculate the target loss? Are there any training hyperparameter recommendations, including lr, etc.?", "扩展字段": {"回复人": "liufangxu", "引用内容": "", "回复时间": "2024-06-27T09:17:43"}}, {"楼ID": "4", "回复": "Yes, both stages of training only calculate the target loss. Detailed info can be found here: https://huggingface.co/BAAI/Infinity-Instruct-3M-0613-Mistral-7B", "扩展字段": {"回复人": "ZacLiu", "引用内容": "two-stage supervised fine-tuning (SFT) on Mistral-7B, Do both stages of training only calculate the target loss? Are there any training hyperparameter recommendations, including lr, etc.?", "回复时间": "2024-06-27T09:52:18"}}, {"楼ID": "5", "回复": "epoch: 3lr: 5e-6min_lr: 0lr_warmup_steps: 40lr_decay_style: cosineweight_decay: 0.0adam_beta1: 0.9adam_beta2: 0.95global_batch_size: 528clip_grad: 1.0Are the training parameters for both stages the same?", "扩展字段": {"回复人": "liufangxu", "引用内容": "", "回复时间": "2024-06-27T10:05:31"}}, {"楼ID": "6", "回复": "Yes, the training parameters for both stages are identical.", "扩展字段": {"回复人": "ZacLiu", "引用内容": "", "回复时间": "2024-06-27T10:51:24"}}, {"楼ID": "7", "回复": "thansk, I have another question:I found that the 3M data is a mixture of Chinese and English. However, Infinity-Instruct-3M-0613-Mistral-7B is an English base and does not support Chinese. Do I need to filter out the Chinese data during training? Will keeping the Chinese data make the model more effective?", "扩展字段": {"回复人": "liufangxu", "引用内容": "", "回复时间": "2024-06-27T11:53:40"}}, {"楼ID": "8", "回复": "It depends. If you're only focusing on the English language, you can filter out the Chinese data.", "扩展字段": {"回复人": "ZacLiu", "引用内容": "", "回复时间": "2024-06-28T03:21:33"}}, {"楼ID": "9", "回复": "ok, thank you", "扩展字段": {"回复人": "liufangxu", "引用内容": "", "回复时间": "2024-06-28T03:42:08"}}], "元数据": {"href": "/datasets/BAAI/Infinity-Instruct/discussions/4"}}, {"ID": "BAAI/Infinity-Instruct-3", "主題": "Did you dedupe this???", "来源": "rombodawg", "回复": [{"楼ID": "0", "回复": "You do realize that my dataset (Replete-AI/code_bagel_hermes-2.5) includes many of the datasets in your list such as:\nSo if you smashed all these dataset together without removing duplicates then you have a big mess on your hands an a really bad set of data. ", "扩展字段": {"回复人": "rombodawg", "引用内容": "", "回复时间": "2024-06-16T04:41:12"}}, {"楼ID": "1", "回复": "We have carefully selected approximately 1 million code and math instructions from a comprehensive collection of around 14 million instructions. While the larger candidate set contains many duplicates, our selected set is free from any duplicates. Detailed information will be provided in our upcoming technical report. Thank you for bringing this to our attention.", "扩展字段": {"回复人": "ZacLiu", "引用内容": "", "回复时间": "2024-06-16T13:36:35"}}, {"楼ID": "2", "回复": "Ok i was just making sure you are doing you guys are doing your data analytics properly", "扩展字段": {"回复人": "rombodawg", "引用内容": "", "回复时间": "2024-06-16T17:12:36"}}], "元数据": {"href": "/datasets/BAAI/Infinity-Instruct/discussions/3"}}, {"ID": "BAAI/Infinity-Instruct-2", "主題": "[bot] Conversion to Parquet", "来源": "parquet-converter", "回复": [{"楼ID": "0", "回复": "The parquet-converter bot has created a version of this dataset in the Parquet format in the refs/convert/parquet branch.\nApache Parquet is a popular columnar storage format known for:\nThis is what powers the dataset viewer on each dataset page and every dataset on the Hub can be accessed with the same code (you can use HF Datasets, ClickHouse, DuckDB, Pandas or Polars, up to you).\nYou can learn more about the advantages associated with Parquet in the documentation.\nYou can access the Parquet version of the dataset by following this link: refs/convert/parquet\nWhen the dataset is already in Parquet format, the data are not converted and the files in refs/convert/parquet are links to the original files. This rule has an exception to ensure the dataset viewer API to stay fast: if the row group size of the original Parquet files is too big, new Parquet files are generated.\nYou don't need to do anything. The Parquet version of the dataset is available for you to use. Refer to the documentation for examples and code snippets on how to query the Parquet files with ClickHouse, DuckDB, Pandas or Polars.\nIf you have any questions or concerns, feel free to ask in the discussion below. You can also close the discussion if you don't have any questions.", "扩展字段": {"回复人": "parquet-converter", "引用内容": "", "回复时间": "2024-06-16T00:10:19"}}], "元数据": {"href": "/datasets/BAAI/Infinity-Instruct/discussions/2"}}, {"ID": "BAAI/Infinity-Instruct-13", "主題": "License", "来源": "sakharamg", "回复": [{"楼ID": "0", "回复": "Thank You for releasing the data. What is the license of it?", "扩展字段": {"回复人": "sakharamg", "引用内容": "", "回复时间": "2024-07-22T03:02:50"}}], "元数据": {"href": "/datasets/BAAI/Infinity-Instruct/discussions/13"}}, {"ID": "BAAI/Infinity-Instruct-12", "主題": "数据有问题，处理的格式不一致，导致最新的版本用不了", "来源": "Amu", "回复": [{"楼ID": "0", "回复": "复现：", "扩展字段": {"回复人": "Amu", "引用内容": "", "回复时间": "2024-07-13T08:16:18"}}, {"楼ID": "1", "回复": "不好意思，我们重新组织了一下数据集。可以参考这份代码加载0625或者3M：\nfrom datasets import load_datasetdata_3M = load_dataset('BAAI/Infinity-Instruct','3M',split='train')data_0625 = load_dataset('BAAI/Infinity-Instruct','0625',split='train')", "扩展字段": {"回复人": "hyxmmm", "引用内容": "", "回复时间": "2024-07-15T03:55:00"}}, {"楼ID": "2", "回复": "可以了 感谢", "扩展字段": {"回复人": "Amu", "引用内容": "", "回复时间": "2024-07-15T12:31:28"}}], "元数据": {"href": "/datasets/BAAI/Infinity-Instruct/discussions/12"}}]
[{"ID": "HuggingFaceH4/ultrachat_200k-5", "主題": "What is the exact difference between train_sft and train_gen?", "来源": "PhilipMay", "回复": [{"楼ID": "0", "回复": "Hi,can you please explain the exact difference between train_sft and train_gen?Many thanksPhilip", "扩展字段": {"回复人": "PhilipMay", "引用内容": "", "回复时间": "2024-03-31T05:22:55"}}, {"楼ID": "1", "回复": "I have the same question.", "扩展字段": {"回复人": "jiangjun0105woven", "引用内容": "", "回复时间": "2024-10-26T07:18:37"}}], "元数据": {"href": "/datasets/HuggingFaceH4/ultrachat_200k/discussions/5"}}, {"ID": "HuggingFaceH4/ultrachat_200k-3", "主題": "Becareful when using this data", "来源": "timlim123", "回复": [{"楼ID": "0", "回复": "just a heads up to anyone wanting to use this data, it has a few problems:\ntake note to filter away this yourself. ", "扩展字段": {"回复人": "timlim123", "引用内容": "", "回复时间": "2023-12-08T07:10:42"}}, {"楼ID": "1", "回复": "some snippet of code to help out:\nfilter away incomplete conversations while looping through the data\nset limit for user query, i think better to do this than retaining all the gibberish text", "扩展字段": {"回复人": "timlim123", "引用内容": "", "回复时间": "2023-12-08T07:12:53"}}, {"楼ID": "2", "回复": "I checked this for train_sft and can absolutely NOT confirm it.", "扩展字段": {"回复人": "PhilipMay", "引用内容": "incomplete conversation (odd number, ending with user query)", "回复时间": "2024-03-31T05:40:39"}}], "元数据": {"href": "/datasets/HuggingFaceH4/ultrachat_200k/discussions/3"}}, {"ID": "HuggingFaceH4/ultrachat_200k-2", "主題": "does this dataset have loading scripts？", "来源": "zyh3826", "回复": [{"楼ID": "0", "回复": "does this dataset have loading scripts?thanks", "扩展字段": {"回复人": "zyh3826", "引用内容": "", "回复时间": "2023-11-17T09:46:57"}}], "元数据": {"href": "/datasets/HuggingFaceH4/ultrachat_200k/discussions/2"}}, {"ID": "HuggingFaceH4/ultrachat_200k-1", "主題": "How was this prompt created and what does it mean??", "来源": "typeof", "回复": [{"楼ID": "0", "回复": "\n@lewtun\n how/what happened to these prompts? 🫠 is there some creative hack/technique here or is it garbage? are you training on instructions also? am I missing something lol?\nhttps://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k/viewer/default/train_sft?p=50&row=5026\nSome other examples:", "扩展字段": {"回复人": "typeof", "引用内容": "", "回复时间": "2023-11-02T21:55:14"}}, {"楼ID": "1", "回复": "I'll defer to the repo authors, but this is a filtered version of the UltraChat dataset which is synthetically generated and incorporates some source material from common-crawl. These are likely just artifacts of their data generation pipeline and junk from common crawl. ", "扩展字段": {"回复人": "blair-johnson", "引用内容": "", "回复时间": "2023-11-06T16:23:55"}}], "元数据": {"href": "/datasets/HuggingFaceH4/ultrachat_200k/discussions/1"}}]
